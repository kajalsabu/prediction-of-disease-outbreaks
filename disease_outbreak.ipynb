{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 1157702,
          "sourceType": "datasetVersion",
          "datasetId": 654897
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kajalsabu/prediction-of-disease-outbreaks/blob/main/disease_outbreak.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "kaushil268_disease_prediction_using_machine_learning_path = kagglehub.dataset_download('kaushil268/disease-prediction-using-machine-learning')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "uWAIdOI2pwR2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2546f87-e3d7-4560-a86e-791d1a924a84"
      },
      "cell_type": "code",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/kaushil268/disease-prediction-using-machine-learning?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29.8k/29.8k [00:00<00:00, 10.5MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Data source import complete.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T11:53:19.661862Z",
          "iopub.execute_input": "2025-02-13T11:53:19.662296Z",
          "iopub.status.idle": "2025-02-13T11:53:19.666867Z",
          "shell.execute_reply.started": "2025-02-13T11:53:19.662262Z",
          "shell.execute_reply": "2025-02-13T11:53:19.665661Z"
        },
        "id": "gC9rYt0qpwR5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook aims to implement a robust machine-learning model that can efficiently predict the disease of a human, based on the symptoms that he/she possesses. Let us look into how we can approach this machine-learning problem:\n",
        "\n",
        "# **Approach**:\n",
        "\n",
        "* **Gathering the Data**: Data preparation is the primary step for any machine learning problem. We will be using a dataset from Kaggle for this problem. This dataset consists of two CSV files one for training and one for testing. There is a total of 133 columns in the dataset out of which 132 columns represent the symptoms and the last column is the prognosis.\n",
        "* **Cleaning the Data**: Cleaning is the most important step in a machine learning project. The quality of our data determines the quality of our machine-learning model. So it is always necessary to clean the data before feeding it to the model for training. In our dataset all the columns are numerical, the target column i.e. prognosis is a string type and is encoded to numerical form using a label encoder.\n",
        "* **Model Building**: After gathering and cleaning the data, the data is ready and can be used to train a machine learning model. We will be using this cleaned data to train the Support Vector Classifier, Naive Bayes Classifier, and Random Forest Classifier. We will be using a confusion matrix to determine the quality of the models.\n",
        "* **Inference**: After training the three models we will be predicting the disease for the input symptoms by combining the predictions of all three models. This makes our overall prediction more robust and accurate.\n",
        "\n",
        "At last, we will be defining a function that takes symptoms separated by commas as input, predicts the disease based on the symptoms by using the trained models, and returns the predictions in a JSON format."
      ],
      "metadata": {
        "id": "loLLgykQpwR5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Implementation**"
      ],
      "metadata": {
        "id": "eM0lvFJjpwR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![DiseasePrediction1000x600.png](attachment:3ed9f1a8-0bc5-4e9a-8773-f371fd25fcd3.png)"
      ],
      "metadata": {
        "id": "JBQFJlGZpwR9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import mode\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "%matplotlib inline"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T09:55:13.004619Z",
          "iopub.execute_input": "2025-02-13T09:55:13.004978Z",
          "iopub.status.idle": "2025-02-13T09:55:13.012205Z",
          "shell.execute_reply.started": "2025-02-13T09:55:13.00495Z",
          "shell.execute_reply": "2025-02-13T09:55:13.01096Z"
        },
        "id": "GQgWL6ZmpwR9"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reading the Dataset\n",
        "\n",
        "After loading the dataset from the folders using the pandas library, while reading the dataset we will be dropping the null column. This dataset is a clean dataset with no null values and all the features consist of 0’s and 1s. Whenever we are solving a classification task it is necessary to check whether our target column is balanced or not. We will be using a bar plot, to check whether the dataset is balanced or not.  "
      ],
      "metadata": {
        "id": "jHDgQ16ypwR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_path = \"/kaggle/input/disease-prediction-using-machine-learning/Training.csv\"\n",
        "\n",
        "data = pd.read_csv(data_path)\n",
        "\n",
        "#data.head()\n",
        "\n",
        "# Reading the train.csv by removing the\n",
        "# last column since it's an empty column\n",
        "data = pd.read_csv(data_path).dropna(axis=1)\n",
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T10:10:35.415577Z",
          "iopub.execute_input": "2025-02-13T10:10:35.415916Z",
          "iopub.status.idle": "2025-02-13T10:10:35.577556Z",
          "shell.execute_reply.started": "2025-02-13T10:10:35.415892Z",
          "shell.execute_reply": "2025-02-13T10:10:35.576397Z"
        },
        "id": "x5C9QIQfpwSA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        },
        "outputId": "3cbce375-e717-4b10-a403-7128aeef6bff"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/kaggle/input/disease-prediction-using-machine-learning/Training.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-3d5cc143f286>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/kaggle/input/disease-prediction-using-machine-learning/Training.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#data.head()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/input/disease-prediction-using-machine-learning/Training.csv'"
          ]
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# checking whether the dataset is balanced or not\n",
        "disease_counts = data['prognosis'].value_counts()\n",
        "\n",
        "temp_df = pd.DataFrame({\n",
        "    \"Disease\": disease_counts.index,\n",
        "    \"Counts\": disease_counts.values\n",
        "})\n",
        "\n",
        "\n",
        "plt.figure(figsize=(18,8))\n",
        "sns.barplot(x='Disease', y = 'Counts' , data=temp_df)\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T10:31:22.577343Z",
          "iopub.execute_input": "2025-02-13T10:31:22.577799Z",
          "iopub.status.idle": "2025-02-13T10:31:23.389431Z",
          "shell.execute_reply.started": "2025-02-13T10:31:22.577702Z",
          "shell.execute_reply": "2025-02-13T10:31:23.388427Z"
        },
        "id": "TX-gY39ypwSD"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above plot, we can observe that the dataset is a balanced dataset i.e. there are exactly 120 samples for each disease, and no further balancing is required. We can notice that our target column i.e. prognosis column is of object datatype, this format is not suitable to train a machine learning model. So, we will be using a label encoder to convert the prognosis column to the numerical datatype. Label Encoder converts the labels into numerical form by assigning a unique index to the labels. If the total number of labels is n, then the numbers assigned to each label will be between 0 to n-1."
      ],
      "metadata": {
        "id": "gfa0gejDpwSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding the target value into numerical values\n",
        "#using LabelEncoder\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "data['prognosis'] = encoder.fit_transform(data['prognosis'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T10:33:20.794558Z",
          "iopub.execute_input": "2025-02-13T10:33:20.79495Z",
          "iopub.status.idle": "2025-02-13T10:33:20.802096Z",
          "shell.execute_reply.started": "2025-02-13T10:33:20.794918Z",
          "shell.execute_reply": "2025-02-13T10:33:20.800801Z"
        },
        "id": "moTlKIc9pwSE"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Similarly for the test data set**"
      ],
      "metadata": {
        "id": "MGL96gYIpwSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_data_path = '/kaggle/input/disease-prediction-using-machine-learning/Testing.csv'\n",
        "\n",
        "#dropping the null column\n",
        "test_data = pd.read_csv(test_data_path).dropna(axis=1)\n",
        "test_data.head()\n",
        "\n",
        "\n",
        "#encoding the target value\n",
        "test_data['prognosis'] = encoder.fit_transform(test_data['prognosis'])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T10:45:36.589334Z",
          "iopub.execute_input": "2025-02-13T10:45:36.58972Z",
          "iopub.status.idle": "2025-02-13T10:45:36.604642Z",
          "shell.execute_reply.started": "2025-02-13T10:45:36.589694Z",
          "shell.execute_reply": "2025-02-13T10:45:36.603516Z"
        },
        "id": "wQv-c6K0pwSF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting the data for training and testing the model\n",
        "\n",
        "Now that we have cleaned our data by removing the Null values and converting the labels to numerical format, It’s time to split the data to train and test the model. We will be splitting the data into 80:20 format i.e. 80% of the dataset will be used for training the model and 20% of the data will be used to evaluate the performance of the models."
      ],
      "metadata": {
        "id": "-B6_ZQUwpwSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data.iloc[:,:-1]\n",
        "y = data.iloc[:, -1]\n",
        "X_train, X_test, y_train, y_test =train_test_split(\n",
        "  X, y, test_size = 0.2, random_state = 24)\n",
        "\n",
        "print(f\"Train: {X_train.shape}, {y_train.shape}\")\n",
        "print(f\"Test: {X_test.shape}, {y_test.shape}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T11:20:24.59581Z",
          "iopub.execute_input": "2025-02-13T11:20:24.596324Z",
          "iopub.status.idle": "2025-02-13T11:20:24.614494Z",
          "shell.execute_reply.started": "2025-02-13T11:20:24.596286Z",
          "shell.execute_reply": "2025-02-13T11:20:24.613286Z"
        },
        "id": "QgDgBkMepwSF"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Building\n",
        "\n",
        "We will be now be working on the modeling part. We will be using K-Fold cross-validation to evaluate the machine-learning models. We will be using Support Vector Classifier, Gaussian Naive Bayes Classifier, and Random Forest Classifier for cross-validation. Before moving into the implementation part let us get familiar with k-fold cross-validation and the machine learning models.\n",
        "\n",
        "* **K-Fold Cross-Validation**: K-Fold cross-validation is one of the cross-validation techniques in which the whole dataset is split into k number of subsets, also known as folds, then training of the model is performed on the k-1 subsets and the remaining one subset is used to evaluate the model performance.\n",
        "  \n",
        "* **Support Vector Classifier**: Support Vector Classifier is a discriminative classifier i.e. when given a labeled training data, the algorithm tries to find an optimal hyperplane that accurately separates the samples into different categories in hyperspace.\n",
        "\n",
        "  \n",
        "* **Gaussian Naive Bayes Classifier**: It is a probabilistic machine learning algorithm that internally uses Bayes Theorem to classify the data points.\n",
        "\n",
        "  \n",
        "* **Random Forest Classifier**: Random Forest is an ensemble learning-based supervised machine learning classification algorithm that internally uses multiple decision trees to make the classification. In a random forest classifier, all the internal decision trees are weak learners, and the outputs of these weak decision trees are combined i.e. mode of all the predictions is as the final prediction."
      ],
      "metadata": {
        "id": "23Rp60tHpwSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using K-Fold Cross-Validation for model selection"
      ],
      "metadata": {
        "id": "RMW8fuEtpwSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining scoring metric for k-fold cross validation\n",
        "def cv_scoring(estimator, X, y):\n",
        "    return accuracy_score(y, estimator.predict(X))\n",
        "\n",
        "# Initializing Models\n",
        "models = {\n",
        "    \"SVC\":SVC(),\n",
        "    \"Gaussian NB\":GaussianNB(),\n",
        "    \"Random Forest\":RandomForestClassifier(random_state=18)\n",
        "}\n",
        "\n",
        "# Producing cross validation score for the models\n",
        "for model_name in models:\n",
        "    model = models[model_name]\n",
        "    scores = cross_val_score(model, X, y, cv = 10,\n",
        "                             n_jobs = -1,\n",
        "                             scoring = cv_scoring)\n",
        "    print(\"==\"*30)\n",
        "    print(model_name)\n",
        "    print(f\"Scores: {scores}\")\n",
        "    print(f\"Mean Score: {np.mean(scores)}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T11:22:43.101295Z",
          "iopub.execute_input": "2025-02-13T11:22:43.101652Z",
          "iopub.status.idle": "2025-02-13T11:22:47.24184Z",
          "shell.execute_reply.started": "2025-02-13T11:22:43.101626Z",
          "shell.execute_reply": "2025-02-13T11:22:47.240652Z"
        },
        "id": "AFbCZwwLpwSG"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above output, we can notice that all our machine learning algorithms are performing very well and the mean scores after k fold cross-validation are also very high. To build a robust model we can combine i.e. take the mode of the predictions of all three models so that even one of the models makes wrong predictions and the other two make correct predictions then the final output would be the correct one. This approach will help us to keep the predictions much more accurate on completely unseen data. In the below code we will be training all the three models on the train data, checking the quality of our models using a confusion matrix, and then combine the predictions of all three models."
      ],
      "metadata": {
        "id": "5bPGQVJ6pwSG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building robust classifier by combining all models:"
      ],
      "metadata": {
        "id": "4w1kxJfhpwSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training and testing SVM Classifier\n",
        "svm_model = SVC()\n",
        "svm_model.fit(X_train, y_train)\n",
        "preds = svm_model.predict(X_test)\n",
        "\n",
        "print(f\"Accuracy on train data by SVM Classifier\\\n",
        ": {accuracy_score(y_train, svm_model.predict(X_train))*100}\")\n",
        "\n",
        "print(f\"Accuracy on test data by SVM Classifier\\\n",
        ": {accuracy_score(y_test, preds)*100}\")\n",
        "cf_matrix = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "plt.title(\"Confusion Matrix for SVM Classifier on Test Data\")\n",
        "plt.show()\n",
        "\n",
        "# Training and testing Naive Bayes Classifier\n",
        "nb_model = GaussianNB()\n",
        "nb_model.fit(X_train, y_train)\n",
        "preds = nb_model.predict(X_test)\n",
        "print(f\"Accuracy on train data by Naive Bayes Classifier\\\n",
        ": {accuracy_score(y_train, nb_model.predict(X_train))*100}\")\n",
        "\n",
        "print(f\"Accuracy on test data by Naive Bayes Classifier\\\n",
        ": {accuracy_score(y_test, preds)*100}\")\n",
        "cf_matrix = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "plt.title(\"Confusion Matrix for Naive Bayes Classifier on Test Data\")\n",
        "plt.show()\n",
        "\n",
        "# Training and testing Random Forest Classifier\n",
        "rf_model = RandomForestClassifier(random_state=18)\n",
        "rf_model.fit(X_train, y_train)\n",
        "preds = rf_model.predict(X_test)\n",
        "print(f\"Accuracy on train data by Random Forest Classifier\\\n",
        ": {accuracy_score(y_train, rf_model.predict(X_train))*100}\")\n",
        "\n",
        "print(f\"Accuracy on test data by Random Forest Classifier\\\n",
        ": {accuracy_score(y_test, preds)*100}\")\n",
        "\n",
        "cf_matrix = confusion_matrix(y_test, preds)\n",
        "plt.figure(figsize=(12,8))\n",
        "sns.heatmap(cf_matrix, annot=True)\n",
        "plt.title(\"Confusion Matrix for Random Forest Classifier on Test Data\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T11:23:12.63559Z",
          "iopub.execute_input": "2025-02-13T11:23:12.635925Z",
          "iopub.status.idle": "2025-02-13T11:23:24.417989Z",
          "shell.execute_reply.started": "2025-02-13T11:23:12.635899Z",
          "shell.execute_reply": "2025-02-13T11:23:24.416722Z"
        },
        "id": "tPJPZvpapwSH"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the above confusion matrices, we can see that the models are performing very well on the unseen data. Now we will be training the models on the whole train data present in the dataset that we downloaded and then test our combined model on test data present in the dataset."
      ],
      "metadata": {
        "id": "DdFKRI-9pwSH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fitting the model on whole data and validating on the Test dataset:"
      ],
      "metadata": {
        "id": "oWYX2RzIpwSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training the models on whole data\n",
        "final_svm_model = SVC()\n",
        "final_nb_model = GaussianNB()\n",
        "final_rf_model = RandomForestClassifier(random_state=18)\n",
        "final_svm_model.fit(X, y)\n",
        "final_nb_model.fit(X, y)\n",
        "final_rf_model.fit(X, y)\n",
        "\n",
        "# Reading the test data\n",
        "test_data = pd.read_csv(\"/kaggle/input/disease-prediction-using-machine-learning/Testing.csv\").dropna(axis=1)\n",
        "\n",
        "test_X = test_data.iloc[:, :-1]\n",
        "test_Y = encoder.transform(test_data.iloc[:, -1])\n",
        "\n",
        "# Making prediction by take mode of predictions\n",
        "# made by all the classifiers\n",
        "svm_preds = final_svm_model.predict(test_X)\n",
        "nb_preds = final_nb_model.predict(test_X)\n",
        "rf_preds = final_rf_model.predict(test_X)\n",
        "\n",
        "#!pip install scipy\n",
        "from scipy import stats\n",
        "\n",
        "final_preds = [stats.mode([i,j,k])[0] for i,j,k in zip(svm_preds, nb_preds, rf_preds)]\n",
        "\n",
        "print(f\"Accuracy on Test dataset by the combined model: {accuracy_score(test_Y, final_preds)*100}\")\n",
        "\n",
        "cf_matrix = confusion_matrix(test_Y, final_preds)\n",
        "plt.figure(figsize=(12,8))\n",
        "\n",
        "sns.heatmap(cf_matrix, annot = True)\n",
        "plt.title(\"Confusion Matrix for Combined Model on Test Dataset\")\n",
        "plt.show()\n",
        "\n",
        "# This code is modified by Susobhan Akhuli\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T11:51:42.718859Z",
          "iopub.execute_input": "2025-02-13T11:51:42.719286Z",
          "iopub.status.idle": "2025-02-13T11:51:47.104734Z",
          "shell.execute_reply.started": "2025-02-13T11:51:42.719255Z",
          "shell.execute_reply": "2025-02-13T11:51:47.103709Z"
        },
        "id": "hPfhV59zpwSI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that our combined model has classified all the data points accurately. We have come to the final part of this whole implementation, we will be creating a function that takes symptoms separated by commas as input and outputs the predicted disease using the combined model based on the input symptoms."
      ],
      "metadata": {
        "id": "tkTFcgy5pwSI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a function that can take symptoms as input and generate predictions for disease"
      ],
      "metadata": {
        "id": "JMM-hPYfpwSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "symptoms = X.columns.values\n",
        "\n",
        "#Creating a symptom index dictionary to encode the input symptoms into numerical form\n",
        "symptom_index = {}\n",
        "for index, value in enumerate(symptoms):\n",
        "    symptom = \" \".join([i.capitalize() for i in value.split(\"_\")])\n",
        "    symptom_index[symptom] = index\n",
        "\n",
        "data_dict = {\n",
        "    \"symptom_index\": symptom_index,\n",
        "    \"predictions_classes\":encoder.classes_\n",
        "}\n",
        "\n",
        "\n",
        "#Defining the functions\n",
        "#Input: string containing symptoms seperated by commas\n",
        "#Output : Generated predictions by models\n",
        "def predictDisease(symptoms):\n",
        "    symptoms = symptoms.split(\",\")\n",
        "\n",
        "#creating input data for the models\n",
        "    input_data = [0] * len(data_dict[\"symptom_index\"])\n",
        "    for symptom in symptoms:\n",
        "        index = data_dict[\"symptom_index\"][symptom]\n",
        "        input_data[index] = 1\n",
        "\n",
        "    input_data = np.array(input_data).reshape(1,-1)\n",
        "\n",
        "#generating individual outputs\n",
        "    rf_predictions = data_dict[\"predictions_classes\"][final_rf_model.predict(input_data)[0]]\n",
        "    nb_predictions = data_dict[\"predictions_classes\"][final_nb_model.predict(input_data)[0]]\n",
        "    svm_predictions = data_dict[\"predictions_classes\"][final_svm_model.predict(input_data)[0]]\n",
        "\n",
        "#making final predictions by taking mode of all predictions\n",
        "#Use statistics.mode instead of scipy.stats.mode\n",
        "    import statistics\n",
        "    final_predictions = statistics.mode([rf_predictions, nb_predictions, svm_predictions])\n",
        "    predictions = {\n",
        "        \"rf_model_prediction\": rf_predictions,\n",
        "        \"naive_bayes_prediction\": nb_predictions,\n",
        "        \"svm_model_prediction\":svm_predictions,\n",
        "        \"final_prediction\": final_predictions\n",
        "    }\n",
        "    return predictions\n",
        "\n",
        "\n",
        "#testing the function\n",
        "print(predictDisease(\"Itching,Skin Rash,Nodal Skin Eruptions\"))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-13T11:54:09.769462Z",
          "iopub.execute_input": "2025-02-13T11:54:09.769823Z",
          "iopub.status.idle": "2025-02-13T11:54:09.790581Z",
          "shell.execute_reply.started": "2025-02-13T11:54:09.769796Z",
          "shell.execute_reply": "2025-02-13T11:54:09.789438Z"
        },
        "id": "hWBi6fcKpwSI"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: The symptoms that are given as input to the function should be exactly the same among the 132 symptoms in the dataset."
      ],
      "metadata": {
        "id": "L78bYh3ApwSJ"
      }
    }
  ]
}